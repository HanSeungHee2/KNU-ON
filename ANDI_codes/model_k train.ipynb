{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "733e3c76",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-19T13:28:59.661545Z",
     "start_time": "2024-07-19T13:28:55.363583Z"
    }
   },
   "outputs": [],
   "source": [
    "# \n",
    "import pickle\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "with open('ANDI_DATA/feature_v2_trainset.pkl', 'rb') as f:\n",
    "    train_dataset = pickle.load(f)\n",
    "    \n",
    "with open('ANDI_DATA/feature_v2_valset.pkl', 'rb') as f:\n",
    "    val_dataset = pickle.load(f)\n",
    "\n",
    "with open('ANDI_DATA/feature_v2_testset.pkl', 'rb') as f:\n",
    "    test_dataset = pickle.load(f)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "131867db",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-19T13:29:03.839905Z",
     "start_time": "2024-07-19T13:29:02.211861Z"
    }
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import reduce_sum\n",
    "\n",
    "def weighted_sum_data_k(\n",
    "    data,\n",
    "    coefficients: list = [0, 0, 1, 0, 1, 0, 0, 0]\n",
    "):\n",
    "\n",
    "    for idx, coefficient in enumerate(coefficients):\n",
    "        data[..., idx] = data[..., idx] * coefficient\n",
    "\n",
    "    data = reduce_sum(\n",
    "        data,\n",
    "        axis=len(data.shape) - 1,\n",
    "        keepdims=True\n",
    "    )\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d8d55304",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-19T13:29:20.561073Z",
     "start_time": "2024-07-19T13:29:03.843163Z"
    }
   },
   "outputs": [],
   "source": [
    "train_input = weighted_sum_data_k(train_dataset['x'])\n",
    "train_input = tf.reshape(train_input,(train_input.shape[0],1,191,1))\n",
    "train_output = train_dataset['k']\n",
    "\n",
    "val_input = weighted_sum_data_k(val_dataset['x'])\n",
    "val_input = tf.reshape(val_input,(val_input.shape[0],1,191,1))\n",
    "val_output = val_dataset['k']\n",
    "\n",
    "test_input = weighted_sum_data_k(test_dataset['x'])\n",
    "test_input = tf.reshape(test_input,(test_input.shape[0],1,191,1))\n",
    "test_output = test_dataset['k']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6bedeb8f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-19T13:29:20.572423Z",
     "start_time": "2024-07-19T13:29:20.564534Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_input shape = (436000, 1, 191, 1)\n",
      "train_output shape = (436000, 200)\n",
      "val_input shape = (116000, 1, 191, 1)\n",
      "val_output shape = (116000, 200)\n",
      "test_input shape = (72000, 1, 191, 1)\n",
      "test_output shape = (72000, 200)\n"
     ]
    }
   ],
   "source": [
    "print('train_input shape = {}'.format(train_input.shape))\n",
    "print('train_output shape = {}'.format(train_output.shape))\n",
    "print('val_input shape = {}'.format(val_input.shape))\n",
    "print('val_output shape = {}'.format(val_output.shape))\n",
    "print('test_input shape = {}'.format(test_input.shape))\n",
    "print('test_output shape = {}'.format(test_output.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "989c7b5f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-19T13:29:27.283714Z",
     "start_time": "2024-07-19T13:29:20.575190Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_1 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_1 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_1 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_2 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_2 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_2 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            [(None, 1, 191, 1)]  0                                            \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed (TimeDistribut (None, 1, 187, 256)  1536        input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization (BatchNorma (None, 1, 187, 256)  1024        time_distributed[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "activation (Activation)         (None, 1, 187, 256)  0           batch_normalization[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_1 (TimeDistrib (None, 1, 93, 256)   0           activation[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_2 (TimeDistrib (None, 1, 89, 512)   655872      time_distributed_1[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_1 (BatchNor (None, 1, 89, 512)   2048        time_distributed_2[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "activation_1 (Activation)       (None, 1, 89, 512)   0           batch_normalization_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_3 (TimeDistrib (None, 1, 44, 512)   0           activation_1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "dropout (Dropout)               (None, 1, 44, 512)   0           time_distributed_3[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_4 (TimeDistrib (None, 1, 40, 1024)  2622464     dropout[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_2 (BatchNor (None, 1, 40, 1024)  4096        time_distributed_4[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "activation_2 (Activation)       (None, 1, 40, 1024)  0           batch_normalization_2[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_5 (TimeDistrib (None, 1, 20, 1024)  0           activation_2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_6 (TimeDistrib (None, 1, 20480)     0           time_distributed_5[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)             (None, 1, 20480)     0           time_distributed_6[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional (Bidirectional)   (None, 1, 2048)      176168960   dropout_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_3 (BatchNor (None, 1, 2048)      8192        bidirectional[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_2 (Dropout)             (None, 1, 2048)      0           batch_normalization_3[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_1 (Bidirectional) (None, 1, 2048)      25174016    dropout_2[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_4 (BatchNor (None, 1, 2048)      8192        bidirectional_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "dropout_3 (Dropout)             (None, 1, 2048)      0           batch_normalization_4[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_2 (Bidirectional) (None, 2048)         25174016    dropout_3[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "reshape (Reshape)               (None, 2048, 1)      0           bidirectional_2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "permute (Permute)               (None, 1, 2048)      0           reshape[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "reshape_1 (Reshape)             (None, 1, 2048)      0           permute[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "attention (Attention)           (None, 1, 2048)      0           reshape_1[0][0]                  \n",
      "                                                                 reshape_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "multiply (Multiply)             (None, 1, 2048)      0           reshape_1[0][0]                  \n",
      "                                                                 attention[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "permute_1 (Permute)             (None, 2048, 1)      0           multiply[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "reshape_2 (Reshape)             (None, 1, 2048)      0           permute_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "flatten_1 (Flatten)             (None, 2048)         0           reshape_2[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_5 (BatchNor (None, 2048)         8192        flatten_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_4 (Dropout)             (None, 2048)         0           batch_normalization_5[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (None, 200)          409800      dropout_4[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 230,238,408\n",
      "Trainable params: 230,222,536\n",
      "Non-trainable params: 15,872\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.layers import Input, Conv1D, BatchNormalization, Activation, MaxPooling1D, Flatten, TimeDistributed\n",
    "from tensorflow.keras.layers import Dropout, Bidirectional, LSTM, Attention, Multiply, Reshape, Permute, Dense\n",
    "from tensorflow.keras import regularizers\n",
    "from tensorflow.keras.backend import clear_session\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau\n",
    "\n",
    "MODEL_SAVE_FOLDER_PATH = 'models/'\n",
    "if not os.path.exists(MODEL_SAVE_FOLDER_PATH):\n",
    "    os.mkdir(MODEL_SAVE_FOLDER_PATH)\n",
    "\n",
    "model_path = MODEL_SAVE_FOLDER_PATH + 'model_k.hdf5' \n",
    "\n",
    "cb_checkpoint = ModelCheckpoint(filepath=model_path, monitor='val_loss', mode = 'min',\\\n",
    "                                verbose=1, save_best_only=True)\n",
    "cb_early_stopping = EarlyStopping(monitor='val_loss', mode = 'min', patience= 10)\n",
    "\n",
    "# Compile model\n",
    "\n",
    "rlr = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience= 5, min_lr=1e-7, \\\n",
    "                        verbose=1, min_delta=1e-5)\n",
    "\n",
    "\n",
    "l1 = 1e-5\n",
    "l2 = 1e-4\n",
    "lr = 1e-4\n",
    "kernel_size=5\n",
    "strides=1\n",
    "activation='relu'\n",
    "\n",
    "inputs = Input(shape=(1, 191, 1))\n",
    "\n",
    "x = TimeDistributed(Conv1D(256, kernel_size=kernel_size, input_shape=(None, 191, 1)))(inputs)\n",
    "x = BatchNormalization()(x)\n",
    "x = Activation('relu')(x)\n",
    "x = TimeDistributed(MaxPooling1D(2))(x)\n",
    "x = TimeDistributed(Conv1D(512, kernel_size=kernel_size))(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = Activation('relu')(x)\n",
    "x = TimeDistributed(MaxPooling1D(2))(x)\n",
    "x = Dropout(0.15)(x)\n",
    "x = TimeDistributed(Conv1D(1024, kernel_size=kernel_size))(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = Activation('relu')(x)\n",
    "x = TimeDistributed(MaxPooling1D(2))(x)\n",
    "x = TimeDistributed(Flatten())(x)\n",
    "x = Dropout(0.25)(x)\n",
    "x = Bidirectional(LSTM(1024, activation='relu', return_sequences=True))(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = Dropout(0.25)(x)\n",
    "x = Bidirectional(LSTM(1024, activation='relu', return_sequences=True))(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = Dropout(0.5)(x)\n",
    "x = Bidirectional(LSTM(1024, activation='relu', return_sequences=False))(x)\n",
    "x = Reshape((2048, 1))(x)\n",
    "x = Permute((2, 1))(x)\n",
    "x = Reshape((-1, 2048))(x)\n",
    "\n",
    "attention_result = Attention()([x, x])\n",
    "x = Multiply()([x, attention_result])\n",
    "\n",
    "x = Permute((2, 1))(x)\n",
    "x = Reshape((-1, 2048))(x)\n",
    "x = Flatten()(x)\n",
    "\n",
    "x = BatchNormalization()(x)\n",
    "x = Dropout(0.5)(x)\n",
    "outputs = Dense(\n",
    "        200,\n",
    "        activation=activation,\n",
    "        kernel_regularizer=regularizers.l1_l2(l1=l1, l2=l2)\n",
    "    )(x)\n",
    "\n",
    "model = Model(inputs=inputs, outputs=outputs)\n",
    "model.summary()\n",
    "\n",
    "\n",
    "model.compile(optimizer= Adam(lr),\n",
    "                  loss='mae',\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa7494be",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-07-19T13:29:10.500Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1000\n",
      "426/426 [==============================] - 72s 151ms/step - loss: 0.6016 - val_loss: 0.6443\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.64434, saving model to models/model_k.hdf5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.8/site-packages/tensorflow/python/keras/utils/generic_utils.py:494: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  warnings.warn('Custom mask layers require a config and must override '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/1000\n",
      "426/426 [==============================] - 63s 149ms/step - loss: 0.3895 - val_loss: 0.3477\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.64434 to 0.34768, saving model to models/model_k.hdf5\n",
      "Epoch 3/1000\n",
      "426/426 [==============================] - 63s 149ms/step - loss: 0.3294 - val_loss: 0.2870\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.34768 to 0.28703, saving model to models/model_k.hdf5\n",
      "Epoch 4/1000\n",
      "426/426 [==============================] - 63s 149ms/step - loss: 0.2913 - val_loss: 0.2595\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.28703 to 0.25952, saving model to models/model_k.hdf5\n",
      "Epoch 5/1000\n",
      "426/426 [==============================] - 63s 149ms/step - loss: 0.2675 - val_loss: 0.2406\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.25952 to 0.24063, saving model to models/model_k.hdf5\n",
      "Epoch 6/1000\n",
      "426/426 [==============================] - 63s 149ms/step - loss: 0.2552 - val_loss: 0.2365\n",
      "\n",
      "Epoch 00006: val_loss improved from 0.24063 to 0.23651, saving model to models/model_k.hdf5\n",
      "Epoch 7/1000\n",
      "426/426 [==============================] - 63s 149ms/step - loss: 0.2494 - val_loss: 0.2346\n",
      "\n",
      "Epoch 00007: val_loss improved from 0.23651 to 0.23463, saving model to models/model_k.hdf5\n",
      "Epoch 8/1000\n",
      "426/426 [==============================] - 63s 149ms/step - loss: 0.2434 - val_loss: 0.2258\n",
      "\n",
      "Epoch 00008: val_loss improved from 0.23463 to 0.22575, saving model to models/model_k.hdf5\n",
      "Epoch 9/1000\n",
      "106/426 [======>.......................] - ETA: 43s - loss: 0.2403"
     ]
    }
   ],
   "source": [
    "history = model.fit(train_input, train_output, epochs = 1000, batch_size= 1024, verbose=1, \\\n",
    "                   validation_data=(val_input, val_output),callbacks=[cb_checkpoint, cb_early_stopping, rlr])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "078151c3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
