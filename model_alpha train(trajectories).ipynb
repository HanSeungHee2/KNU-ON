{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bb1c2d9e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-11T06:05:11.243683Z",
     "start_time": "2024-12-11T06:05:08.742334Z"
    }
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.layers import Input, Conv1D, BatchNormalization, Activation, MaxPooling1D, Flatten, TimeDistributed\n",
    "from tensorflow.keras.layers import Dropout, Bidirectional, LSTM, Attention, Multiply, Reshape, Permute, Dense\n",
    "from tensorflow.keras import regularizers\n",
    "from tensorflow.keras.backend import clear_session\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau\n",
    "import pickle\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "\n",
    "T = 200\n",
    "tau = 3\n",
    "retype_dimension = int(T-tau+1)\n",
    "\n",
    "\n",
    "with open('Data/Feature/alpha_data (200, {})'.format(tau), 'rb') as f:\n",
    "    alpha_data = pickle.load(f)\n",
    "\n",
    "with open(\"Data/Feature/Trajectories ({})\".format(T),\"rb\") as f:\n",
    "    trajectories = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e1b37adf",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-11T06:05:11.248899Z",
     "start_time": "2024-12-11T06:05:11.246103Z"
    }
   },
   "outputs": [],
   "source": [
    "input_data = trajectories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6948d9e5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-11T06:05:12.157055Z",
     "start_time": "2024-12-11T06:05:11.253008Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_input shape = (300000, 200, 2)\n",
      "train_output shape = (300000, 200)\n",
      "val_input shape = (60000, 200, 2)\n",
      "val_output shape = (60000, 200)\n"
     ]
    }
   ],
   "source": [
    "input_data = trajectories\n",
    "train_input = np.zeros((300000,T,2))\n",
    "val_input = np.zeros((60000,T,2))\n",
    "test_input = np.zeros((60000,T,2))\n",
    "\n",
    "train_output = np.zeros((300000,T))\n",
    "val_output = np.zeros((60000,T))\n",
    "test_output = np.zeros((60000,T))\n",
    "\n",
    "cases = ['single', 'multi', 'immobile', 'dimerization','confinement']\n",
    "\n",
    "\n",
    "for case_num, case in enumerate(cases):\n",
    "    ns_t = int(case_num*60000)\n",
    "    ne_t = int((case_num+1)*60000)\n",
    "\n",
    "    ns_v = int(case_num*12000)\n",
    "    ne_v = int((case_num+1)*12000)\n",
    "\n",
    "    train_input[ns_t:ne_t,:,0] = input_data['x'][case][0]\n",
    "    train_input[ns_t:ne_t,:,1] = input_data['y'][case][0]\n",
    "    val_input[ns_v:ne_v,:,0] = input_data['x'][case][1]\n",
    "    val_input[ns_v:ne_v,:,1] = input_data['y'][case][1]\n",
    "    test_input[ns_v:ne_v,:,0] = input_data['x'][case][2]\n",
    "    test_input[ns_v:ne_v,:,1] = input_data['y'][case][2]\n",
    "    \n",
    "    train_output[ns_t:ne_t] = alpha_data[case][0]\n",
    "    val_output[ns_v:ne_v] = alpha_data[case][1]\n",
    "    test_output[ns_v:ne_v] = alpha_data[case][2]\n",
    "        \n",
    "        \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "print('train_input shape = {}'.format(train_input.shape))\n",
    "print('train_output shape = {}'.format(train_output.shape))\n",
    "print('val_input shape = {}'.format(val_input.shape))\n",
    "print('val_output shape = {}'.format(val_output.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c4bd6f6a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-11T06:05:12.167228Z",
     "start_time": "2024-12-11T06:05:12.161820Z"
    }
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.callbacks import Callback\n",
    "\n",
    "class ValLossHistory(Callback):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.val_loss_list = []\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        # logs['val_loss']를 리스트에 추가\n",
    "        if logs is not None and 'val_loss' in logs:\n",
    "            self.val_loss_list.append(logs['val_loss'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e842c6ec",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-11T23:38:42.999922Z",
     "start_time": "2024-12-11T06:05:33.040640Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input1 (InputLayer)          [(None, 200, 2)]          0         \n",
      "_________________________________________________________________\n",
      "conv1d (Conv1D)              (None, 200, 512)          8704      \n",
      "_________________________________________________________________\n",
      "batch_normalization (BatchNo (None, 200, 512)          2048      \n",
      "_________________________________________________________________\n",
      "conv1d_1 (Conv1D)            (None, 200, 512)          2097664   \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 200, 512)          2048      \n",
      "_________________________________________________________________\n",
      "conv1d_2 (Conv1D)            (None, 200, 512)          2097664   \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 200, 512)          2048      \n",
      "_________________________________________________________________\n",
      "bidirectional (Bidirectional (None, 200, 2048)         12591104  \n",
      "_________________________________________________________________\n",
      "batch_normalization_3 (Batch (None, 200, 2048)         8192      \n",
      "_________________________________________________________________\n",
      "bidirectional_1 (Bidirection (None, 200, 1024)         10489856  \n",
      "_________________________________________________________________\n",
      "batch_normalization_4 (Batch (None, 200, 1024)         4096      \n",
      "_________________________________________________________________\n",
      "bidirectional_2 (Bidirection (None, 512)               2623488   \n",
      "_________________________________________________________________\n",
      "batch_normalization_5 (Batch (None, 512)               2048      \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 200)               102600    \n",
      "=================================================================\n",
      "Total params: 30,031,560\n",
      "Trainable params: 30,021,320\n",
      "Non-trainable params: 10,240\n",
      "_________________________________________________________________\n",
      "Epoch 1/1000\n",
      "  6/600 [..............................] - ETA: 9:14 - loss: 0.7689WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.2745s vs `on_train_batch_end` time: 0.5491s). Check your callbacks.\n",
      "600/600 [==============================] - 620s 1s/step - loss: 0.3476 - val_loss: 0.4883\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.48825, saving model to Models/trajectories_model_alpha_(12/05) (T = 200, ks = 8, 3_cnn, causal).hdf5\n",
      "Epoch 2/1000\n",
      "600/600 [==============================] - 613s 1s/step - loss: 0.2793 - val_loss: 0.2673\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.48825 to 0.26731, saving model to Models/trajectories_model_alpha_(12/05) (T = 200, ks = 8, 3_cnn, causal).hdf5\n",
      "Epoch 3/1000\n",
      "600/600 [==============================] - 613s 1s/step - loss: 0.2651 - val_loss: 0.3126\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 0.26731\n",
      "Epoch 4/1000\n",
      "600/600 [==============================] - 613s 1s/step - loss: 0.2337 - val_loss: 0.2123\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.26731 to 0.21232, saving model to Models/trajectories_model_alpha_(12/05) (T = 200, ks = 8, 3_cnn, causal).hdf5\n",
      "Epoch 5/1000\n",
      "600/600 [==============================] - 613s 1s/step - loss: 0.1830 - val_loss: 0.1689\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.21232 to 0.16893, saving model to Models/trajectories_model_alpha_(12/05) (T = 200, ks = 8, 3_cnn, causal).hdf5\n",
      "Epoch 6/1000\n",
      "600/600 [==============================] - 613s 1s/step - loss: 0.1580 - val_loss: 0.1505\n",
      "\n",
      "Epoch 00006: val_loss improved from 0.16893 to 0.15050, saving model to Models/trajectories_model_alpha_(12/05) (T = 200, ks = 8, 3_cnn, causal).hdf5\n",
      "Epoch 7/1000\n",
      "600/600 [==============================] - 613s 1s/step - loss: 0.1460 - val_loss: 0.1464\n",
      "\n",
      "Epoch 00007: val_loss improved from 0.15050 to 0.14638, saving model to Models/trajectories_model_alpha_(12/05) (T = 200, ks = 8, 3_cnn, causal).hdf5\n",
      "Epoch 8/1000\n",
      "600/600 [==============================] - 612s 1s/step - loss: 0.1363 - val_loss: 0.1352\n",
      "\n",
      "Epoch 00008: val_loss improved from 0.14638 to 0.13523, saving model to Models/trajectories_model_alpha_(12/05) (T = 200, ks = 8, 3_cnn, causal).hdf5\n",
      "Epoch 9/1000\n",
      "600/600 [==============================] - 612s 1s/step - loss: 0.1308 - val_loss: 0.1236\n",
      "\n",
      "Epoch 00009: val_loss improved from 0.13523 to 0.12362, saving model to Models/trajectories_model_alpha_(12/05) (T = 200, ks = 8, 3_cnn, causal).hdf5\n",
      "Epoch 10/1000\n",
      "600/600 [==============================] - 612s 1s/step - loss: 0.1256 - val_loss: 0.1223\n",
      "\n",
      "Epoch 00010: val_loss improved from 0.12362 to 0.12230, saving model to Models/trajectories_model_alpha_(12/05) (T = 200, ks = 8, 3_cnn, causal).hdf5\n",
      "Epoch 11/1000\n",
      "600/600 [==============================] - 612s 1s/step - loss: 0.1207 - val_loss: 0.1115\n",
      "\n",
      "Epoch 00011: val_loss improved from 0.12230 to 0.11154, saving model to Models/trajectories_model_alpha_(12/05) (T = 200, ks = 8, 3_cnn, causal).hdf5\n",
      "Epoch 12/1000\n",
      "600/600 [==============================] - 612s 1s/step - loss: 0.1236 - val_loss: 0.1145\n",
      "\n",
      "Epoch 00012: val_loss did not improve from 0.11154\n",
      "Epoch 13/1000\n",
      "600/600 [==============================] - 613s 1s/step - loss: 0.1237 - val_loss: 0.1173\n",
      "\n",
      "Epoch 00013: val_loss did not improve from 0.11154\n",
      "\n",
      "Epoch 00013: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "Epoch 14/1000\n",
      "600/600 [==============================] - 612s 1s/step - loss: 0.1081 - val_loss: 0.0990\n",
      "\n",
      "Epoch 00014: val_loss improved from 0.11154 to 0.09902, saving model to Models/trajectories_model_alpha_(12/05) (T = 200, ks = 8, 3_cnn, causal).hdf5\n",
      "Epoch 15/1000\n",
      "600/600 [==============================] - 612s 1s/step - loss: 0.1030 - val_loss: 0.0970\n",
      "\n",
      "Epoch 00015: val_loss improved from 0.09902 to 0.09702, saving model to Models/trajectories_model_alpha_(12/05) (T = 200, ks = 8, 3_cnn, causal).hdf5\n",
      "Epoch 16/1000\n",
      "600/600 [==============================] - 613s 1s/step - loss: 0.0994 - val_loss: 0.0936\n",
      "\n",
      "Epoch 00016: val_loss improved from 0.09702 to 0.09358, saving model to Models/trajectories_model_alpha_(12/05) (T = 200, ks = 8, 3_cnn, causal).hdf5\n",
      "Epoch 17/1000\n",
      "600/600 [==============================] - 612s 1s/step - loss: 0.0963 - val_loss: 0.0975\n",
      "\n",
      "Epoch 00017: val_loss did not improve from 0.09358\n",
      "Epoch 18/1000\n",
      "600/600 [==============================] - 613s 1s/step - loss: 0.0946 - val_loss: 0.0908\n",
      "\n",
      "Epoch 00018: val_loss improved from 0.09358 to 0.09083, saving model to Models/trajectories_model_alpha_(12/05) (T = 200, ks = 8, 3_cnn, causal).hdf5\n",
      "Epoch 19/1000\n",
      "600/600 [==============================] - 613s 1s/step - loss: 0.0923 - val_loss: 0.0897\n",
      "\n",
      "Epoch 00019: val_loss improved from 0.09083 to 0.08968, saving model to Models/trajectories_model_alpha_(12/05) (T = 200, ks = 8, 3_cnn, causal).hdf5\n",
      "Epoch 20/1000\n",
      "600/600 [==============================] - 613s 1s/step - loss: 0.0895 - val_loss: 0.0889\n",
      "\n",
      "Epoch 00020: val_loss improved from 0.08968 to 0.08895, saving model to Models/trajectories_model_alpha_(12/05) (T = 200, ks = 8, 3_cnn, causal).hdf5\n",
      "Epoch 21/1000\n",
      "600/600 [==============================] - 613s 1s/step - loss: 0.0877 - val_loss: 0.0848\n",
      "\n",
      "Epoch 00021: val_loss improved from 0.08895 to 0.08483, saving model to Models/trajectories_model_alpha_(12/05) (T = 200, ks = 8, 3_cnn, causal).hdf5\n",
      "Epoch 22/1000\n",
      "600/600 [==============================] - 613s 1s/step - loss: 0.0870 - val_loss: 0.0853\n",
      "\n",
      "Epoch 00022: val_loss did not improve from 0.08483\n",
      "Epoch 23/1000\n",
      "600/600 [==============================] - 613s 1s/step - loss: 0.0843 - val_loss: 0.0836\n",
      "\n",
      "Epoch 00023: val_loss improved from 0.08483 to 0.08356, saving model to Models/trajectories_model_alpha_(12/05) (T = 200, ks = 8, 3_cnn, causal).hdf5\n",
      "Epoch 24/1000\n",
      "600/600 [==============================] - 612s 1s/step - loss: 0.0828 - val_loss: 0.0821\n",
      "\n",
      "Epoch 00024: val_loss improved from 0.08356 to 0.08211, saving model to Models/trajectories_model_alpha_(12/05) (T = 200, ks = 8, 3_cnn, causal).hdf5\n",
      "Epoch 25/1000\n",
      "600/600 [==============================] - 612s 1s/step - loss: 0.0815 - val_loss: 0.0807\n",
      "\n",
      "Epoch 00025: val_loss improved from 0.08211 to 0.08068, saving model to Models/trajectories_model_alpha_(12/05) (T = 200, ks = 8, 3_cnn, causal).hdf5\n",
      "Epoch 26/1000\n",
      "600/600 [==============================] - 613s 1s/step - loss: 0.0800 - val_loss: 0.0812\n",
      "\n",
      "Epoch 00026: val_loss did not improve from 0.08068\n",
      "Epoch 27/1000\n",
      "600/600 [==============================] - 613s 1s/step - loss: 0.0791 - val_loss: 0.0800\n",
      "\n",
      "Epoch 00027: val_loss improved from 0.08068 to 0.08001, saving model to Models/trajectories_model_alpha_(12/05) (T = 200, ks = 8, 3_cnn, causal).hdf5\n",
      "Epoch 28/1000\n",
      "600/600 [==============================] - 612s 1s/step - loss: 0.0793 - val_loss: 0.0790\n",
      "\n",
      "Epoch 00028: val_loss improved from 0.08001 to 0.07899, saving model to Models/trajectories_model_alpha_(12/05) (T = 200, ks = 8, 3_cnn, causal).hdf5\n",
      "Epoch 29/1000\n",
      "600/600 [==============================] - 612s 1s/step - loss: 0.0773 - val_loss: 0.0780\n",
      "\n",
      "Epoch 00029: val_loss improved from 0.07899 to 0.07804, saving model to Models/trajectories_model_alpha_(12/05) (T = 200, ks = 8, 3_cnn, causal).hdf5\n",
      "Epoch 30/1000\n",
      "600/600 [==============================] - 613s 1s/step - loss: 0.0764 - val_loss: 0.0767\n",
      "\n",
      "Epoch 00030: val_loss improved from 0.07804 to 0.07673, saving model to Models/trajectories_model_alpha_(12/05) (T = 200, ks = 8, 3_cnn, causal).hdf5\n",
      "Epoch 31/1000\n",
      "600/600 [==============================] - 613s 1s/step - loss: 0.0750 - val_loss: 0.0768\n",
      "\n",
      "Epoch 00031: val_loss did not improve from 0.07673\n",
      "Epoch 32/1000\n",
      "600/600 [==============================] - 612s 1s/step - loss: 0.0748 - val_loss: 0.0754\n",
      "\n",
      "Epoch 00032: val_loss improved from 0.07673 to 0.07541, saving model to Models/trajectories_model_alpha_(12/05) (T = 200, ks = 8, 3_cnn, causal).hdf5\n",
      "Epoch 33/1000\n",
      "600/600 [==============================] - 613s 1s/step - loss: 0.0732 - val_loss: 0.0763\n",
      "\n",
      "Epoch 00033: val_loss did not improve from 0.07541\n",
      "Epoch 34/1000\n",
      "600/600 [==============================] - 612s 1s/step - loss: 0.0744 - val_loss: 0.0751\n",
      "\n",
      "Epoch 00034: val_loss improved from 0.07541 to 0.07508, saving model to Models/trajectories_model_alpha_(12/05) (T = 200, ks = 8, 3_cnn, causal).hdf5\n",
      "Epoch 35/1000\n",
      "600/600 [==============================] - 612s 1s/step - loss: 0.0721 - val_loss: 0.0777\n",
      "\n",
      "Epoch 00035: val_loss did not improve from 0.07508\n",
      "Epoch 36/1000\n",
      "600/600 [==============================] - 612s 1s/step - loss: 0.0710 - val_loss: 0.0741\n",
      "\n",
      "Epoch 00036: val_loss improved from 0.07508 to 0.07408, saving model to Models/trajectories_model_alpha_(12/05) (T = 200, ks = 8, 3_cnn, causal).hdf5\n",
      "Epoch 37/1000\n",
      "600/600 [==============================] - 613s 1s/step - loss: 0.0701 - val_loss: 0.0734\n",
      "\n",
      "Epoch 00037: val_loss improved from 0.07408 to 0.07339, saving model to Models/trajectories_model_alpha_(12/05) (T = 200, ks = 8, 3_cnn, causal).hdf5\n",
      "Epoch 38/1000\n",
      "600/600 [==============================] - 612s 1s/step - loss: 0.0698 - val_loss: 0.0741\n",
      "\n",
      "Epoch 00038: val_loss did not improve from 0.07339\n",
      "Epoch 39/1000\n",
      "600/600 [==============================] - 612s 1s/step - loss: 0.0694 - val_loss: 0.0714\n",
      "\n",
      "Epoch 00039: val_loss improved from 0.07339 to 0.07141, saving model to Models/trajectories_model_alpha_(12/05) (T = 200, ks = 8, 3_cnn, causal).hdf5\n",
      "Epoch 40/1000\n",
      "600/600 [==============================] - 612s 1s/step - loss: 0.0684 - val_loss: 0.0728\n",
      "\n",
      "Epoch 00040: val_loss did not improve from 0.07141\n",
      "Epoch 41/1000\n",
      "600/600 [==============================] - 612s 1s/step - loss: 0.0676 - val_loss: 0.0730\n",
      "\n",
      "Epoch 00041: val_loss did not improve from 0.07141\n",
      "\n",
      "Epoch 00041: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "Epoch 42/1000\n",
      "600/600 [==============================] - 612s 1s/step - loss: 0.0637 - val_loss: 0.0675\n",
      "\n",
      "Epoch 00042: val_loss improved from 0.07141 to 0.06749, saving model to Models/trajectories_model_alpha_(12/05) (T = 200, ks = 8, 3_cnn, causal).hdf5\n",
      "Epoch 43/1000\n",
      "600/600 [==============================] - 612s 1s/step - loss: 0.0622 - val_loss: 0.0655\n",
      "\n",
      "Epoch 00043: val_loss improved from 0.06749 to 0.06549, saving model to Models/trajectories_model_alpha_(12/05) (T = 200, ks = 8, 3_cnn, causal).hdf5\n",
      "Epoch 44/1000\n",
      "600/600 [==============================] - 611s 1s/step - loss: 0.0613 - val_loss: 0.0670\n",
      "\n",
      "Epoch 00044: val_loss did not improve from 0.06549\n",
      "Epoch 45/1000\n",
      "600/600 [==============================] - 612s 1s/step - loss: 0.0625 - val_loss: 0.0654\n",
      "\n",
      "Epoch 00045: val_loss improved from 0.06549 to 0.06541, saving model to Models/trajectories_model_alpha_(12/05) (T = 200, ks = 8, 3_cnn, causal).hdf5\n",
      "Epoch 46/1000\n",
      "600/600 [==============================] - 612s 1s/step - loss: 0.0604 - val_loss: 0.0651\n",
      "\n",
      "Epoch 00046: val_loss improved from 0.06541 to 0.06509, saving model to Models/trajectories_model_alpha_(12/05) (T = 200, ks = 8, 3_cnn, causal).hdf5\n",
      "Epoch 47/1000\n",
      "600/600 [==============================] - 612s 1s/step - loss: 0.0599 - val_loss: 0.0670\n",
      "\n",
      "Epoch 00047: val_loss did not improve from 0.06509\n",
      "Epoch 48/1000\n",
      "600/600 [==============================] - 612s 1s/step - loss: 0.0596 - val_loss: 0.0643\n",
      "\n",
      "Epoch 00048: val_loss improved from 0.06509 to 0.06427, saving model to Models/trajectories_model_alpha_(12/05) (T = 200, ks = 8, 3_cnn, causal).hdf5\n",
      "Epoch 49/1000\n",
      "600/600 [==============================] - 612s 1s/step - loss: 0.0590 - val_loss: 0.0661\n",
      "\n",
      "Epoch 00049: val_loss did not improve from 0.06427\n",
      "Epoch 50/1000\n",
      "600/600 [==============================] - 612s 1s/step - loss: 0.0584 - val_loss: 0.0640\n",
      "\n",
      "Epoch 00050: val_loss improved from 0.06427 to 0.06398, saving model to Models/trajectories_model_alpha_(12/05) (T = 200, ks = 8, 3_cnn, causal).hdf5\n",
      "Epoch 51/1000\n",
      "600/600 [==============================] - 612s 1s/step - loss: 0.0583 - val_loss: 0.0649\n",
      "\n",
      "Epoch 00051: val_loss did not improve from 0.06398\n",
      "Epoch 52/1000\n",
      "600/600 [==============================] - 612s 1s/step - loss: 0.0580 - val_loss: 0.0648\n",
      "\n",
      "Epoch 00052: val_loss did not improve from 0.06398\n",
      "\n",
      "Epoch 00052: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
      "Epoch 53/1000\n",
      "600/600 [==============================] - 612s 1s/step - loss: 0.0559 - val_loss: 0.0619\n",
      "\n",
      "Epoch 00053: val_loss improved from 0.06398 to 0.06186, saving model to Models/trajectories_model_alpha_(12/05) (T = 200, ks = 8, 3_cnn, causal).hdf5\n",
      "Epoch 54/1000\n",
      "600/600 [==============================] - 612s 1s/step - loss: 0.0552 - val_loss: 0.0613\n",
      "\n",
      "Epoch 00054: val_loss improved from 0.06186 to 0.06126, saving model to Models/trajectories_model_alpha_(12/05) (T = 200, ks = 8, 3_cnn, causal).hdf5\n",
      "Epoch 55/1000\n",
      "600/600 [==============================] - 613s 1s/step - loss: 0.0547 - val_loss: 0.0612\n",
      "\n",
      "Epoch 00055: val_loss improved from 0.06126 to 0.06120, saving model to Models/trajectories_model_alpha_(12/05) (T = 200, ks = 8, 3_cnn, causal).hdf5\n",
      "Epoch 56/1000\n",
      "600/600 [==============================] - 613s 1s/step - loss: 0.0544 - val_loss: 0.0626\n",
      "\n",
      "Epoch 00056: val_loss did not improve from 0.06120\n",
      "Epoch 57/1000\n",
      "600/600 [==============================] - 613s 1s/step - loss: 0.0544 - val_loss: 0.0617\n",
      "\n",
      "Epoch 00057: val_loss did not improve from 0.06120\n",
      "\n",
      "Epoch 00057: ReduceLROnPlateau reducing learning rate to 6.25000029685907e-05.\n",
      "Epoch 58/1000\n",
      "600/600 [==============================] - 613s 1s/step - loss: 0.0532 - val_loss: 0.0602\n",
      "\n",
      "Epoch 00058: val_loss improved from 0.06120 to 0.06016, saving model to Models/trajectories_model_alpha_(12/05) (T = 200, ks = 8, 3_cnn, causal).hdf5\n",
      "Epoch 59/1000\n",
      "600/600 [==============================] - 612s 1s/step - loss: 0.0529 - val_loss: 0.0603\n",
      "\n",
      "Epoch 00059: val_loss did not improve from 0.06016\n",
      "Epoch 60/1000\n",
      "600/600 [==============================] - 612s 1s/step - loss: 0.0526 - val_loss: 0.0602\n",
      "\n",
      "Epoch 00060: val_loss did not improve from 0.06016\n",
      "\n",
      "Epoch 00060: ReduceLROnPlateau reducing learning rate to 3.125000148429535e-05.\n",
      "Epoch 61/1000\n",
      "600/600 [==============================] - 613s 1s/step - loss: 0.0521 - val_loss: 0.0598\n",
      "\n",
      "Epoch 00061: val_loss improved from 0.06016 to 0.05977, saving model to Models/trajectories_model_alpha_(12/05) (T = 200, ks = 8, 3_cnn, causal).hdf5\n",
      "Epoch 62/1000\n",
      "600/600 [==============================] - 612s 1s/step - loss: 0.0519 - val_loss: 0.0594\n",
      "\n",
      "Epoch 00062: val_loss improved from 0.05977 to 0.05944, saving model to Models/trajectories_model_alpha_(12/05) (T = 200, ks = 8, 3_cnn, causal).hdf5\n",
      "Epoch 63/1000\n",
      "600/600 [==============================] - 612s 1s/step - loss: 0.0518 - val_loss: 0.0593\n",
      "\n",
      "Epoch 00063: val_loss improved from 0.05944 to 0.05926, saving model to Models/trajectories_model_alpha_(12/05) (T = 200, ks = 8, 3_cnn, causal).hdf5\n",
      "Epoch 64/1000\n",
      "600/600 [==============================] - 612s 1s/step - loss: 0.0516 - val_loss: 0.0594\n",
      "\n",
      "Epoch 00064: val_loss did not improve from 0.05926\n",
      "Epoch 65/1000\n",
      "600/600 [==============================] - 612s 1s/step - loss: 0.0515 - val_loss: 0.0595\n",
      "\n",
      "Epoch 00065: val_loss did not improve from 0.05926\n",
      "\n",
      "Epoch 00065: ReduceLROnPlateau reducing learning rate to 1.5625000742147677e-05.\n",
      "Epoch 66/1000\n",
      "600/600 [==============================] - 612s 1s/step - loss: 0.0513 - val_loss: 0.0590\n",
      "\n",
      "Epoch 00066: val_loss improved from 0.05926 to 0.05896, saving model to Models/trajectories_model_alpha_(12/05) (T = 200, ks = 8, 3_cnn, causal).hdf5\n",
      "Epoch 67/1000\n",
      "600/600 [==============================] - 612s 1s/step - loss: 0.0512 - val_loss: 0.0589\n",
      "\n",
      "Epoch 00067: val_loss improved from 0.05896 to 0.05890, saving model to Models/trajectories_model_alpha_(12/05) (T = 200, ks = 8, 3_cnn, causal).hdf5\n",
      "Epoch 68/1000\n",
      "600/600 [==============================] - 612s 1s/step - loss: 0.0511 - val_loss: 0.0590\n",
      "\n",
      "Epoch 00068: val_loss did not improve from 0.05890\n",
      "Epoch 69/1000\n",
      "600/600 [==============================] - 613s 1s/step - loss: 0.0510 - val_loss: 0.0590\n",
      "\n",
      "Epoch 00069: val_loss did not improve from 0.05890\n",
      "\n",
      "Epoch 00069: ReduceLROnPlateau reducing learning rate to 7.812500371073838e-06.\n",
      "Epoch 70/1000\n",
      "600/600 [==============================] - 613s 1s/step - loss: 0.0509 - val_loss: 0.0591\n",
      "\n",
      "Epoch 00070: val_loss did not improve from 0.05890\n",
      "Epoch 71/1000\n",
      "600/600 [==============================] - 612s 1s/step - loss: 0.0509 - val_loss: 0.0590\n",
      "\n",
      "Epoch 00071: val_loss did not improve from 0.05890\n",
      "\n",
      "Epoch 00071: ReduceLROnPlateau reducing learning rate to 3.906250185536919e-06.\n",
      "Epoch 72/1000\n",
      "600/600 [==============================] - 612s 1s/step - loss: 0.0508 - val_loss: 0.0589\n",
      "\n",
      "Epoch 00072: val_loss improved from 0.05890 to 0.05887, saving model to Models/trajectories_model_alpha_(12/05) (T = 200, ks = 8, 3_cnn, causal).hdf5\n",
      "Epoch 73/1000\n",
      "600/600 [==============================] - 613s 1s/step - loss: 0.0508 - val_loss: 0.0588\n",
      "\n",
      "Epoch 00073: val_loss improved from 0.05887 to 0.05879, saving model to Models/trajectories_model_alpha_(12/05) (T = 200, ks = 8, 3_cnn, causal).hdf5\n",
      "Epoch 74/1000\n",
      "600/600 [==============================] - 613s 1s/step - loss: 0.0507 - val_loss: 0.0589\n",
      "\n",
      "Epoch 00074: val_loss did not improve from 0.05879\n",
      "Epoch 75/1000\n",
      "600/600 [==============================] - 613s 1s/step - loss: 0.0507 - val_loss: 0.0589\n",
      "\n",
      "Epoch 00075: val_loss did not improve from 0.05879\n",
      "\n",
      "Epoch 00075: ReduceLROnPlateau reducing learning rate to 1.9531250927684596e-06.\n",
      "Epoch 76/1000\n",
      "600/600 [==============================] - 612s 1s/step - loss: 0.0507 - val_loss: 0.0589\n",
      "\n",
      "Epoch 00076: val_loss did not improve from 0.05879\n",
      "Epoch 77/1000\n",
      "600/600 [==============================] - 612s 1s/step - loss: 0.0507 - val_loss: 0.0588\n",
      "\n",
      "Epoch 00077: val_loss did not improve from 0.05879\n",
      "\n",
      "Epoch 00077: ReduceLROnPlateau reducing learning rate to 9.765625463842298e-07.\n",
      "Epoch 78/1000\n",
      "600/600 [==============================] - 612s 1s/step - loss: 0.0507 - val_loss: 0.0589\n",
      "\n",
      "Epoch 00078: val_loss did not improve from 0.05879\n",
      "Epoch 79/1000\n",
      "600/600 [==============================] - 612s 1s/step - loss: 0.0507 - val_loss: 0.0588\n",
      "\n",
      "Epoch 00079: val_loss did not improve from 0.05879\n",
      "\n",
      "Epoch 00079: ReduceLROnPlateau reducing learning rate to 4.882812731921149e-07.\n",
      "Epoch 80/1000\n",
      "600/600 [==============================] - 612s 1s/step - loss: 0.0506 - val_loss: 0.0588\n",
      "\n",
      "Epoch 00080: val_loss did not improve from 0.05879\n",
      "Epoch 81/1000\n",
      "600/600 [==============================] - 612s 1s/step - loss: 0.0506 - val_loss: 0.0588\n",
      "\n",
      "Epoch 00081: val_loss did not improve from 0.05879\n",
      "\n",
      "Epoch 00081: ReduceLROnPlateau reducing learning rate to 2.4414063659605745e-07.\n",
      "Epoch 82/1000\n",
      "600/600 [==============================] - 613s 1s/step - loss: 0.0506 - val_loss: 0.0589\n",
      "\n",
      "Epoch 00082: val_loss did not improve from 0.05879\n",
      "Epoch 83/1000\n",
      "600/600 [==============================] - 613s 1s/step - loss: 0.0506 - val_loss: 0.0587\n",
      "\n",
      "Epoch 00083: val_loss improved from 0.05879 to 0.05875, saving model to Models/trajectories_model_alpha_(12/05) (T = 200, ks = 8, 3_cnn, causal).hdf5\n",
      "Epoch 84/1000\n",
      "600/600 [==============================] - 612s 1s/step - loss: 0.0506 - val_loss: 0.0588\n",
      "\n",
      "Epoch 00084: val_loss did not improve from 0.05875\n",
      "Epoch 85/1000\n",
      "600/600 [==============================] - 612s 1s/step - loss: 0.0506 - val_loss: 0.0589\n",
      "\n",
      "Epoch 00085: val_loss did not improve from 0.05875\n",
      "\n",
      "Epoch 00085: ReduceLROnPlateau reducing learning rate to 1.2207031829802872e-07.\n",
      "Epoch 86/1000\n",
      "600/600 [==============================] - 612s 1s/step - loss: 0.0506 - val_loss: 0.0588\n",
      "\n",
      "Epoch 00086: val_loss did not improve from 0.05875\n",
      "Epoch 87/1000\n",
      "600/600 [==============================] - 612s 1s/step - loss: 0.0506 - val_loss: 0.0588\n",
      "\n",
      "Epoch 00087: val_loss did not improve from 0.05875\n",
      "\n",
      "Epoch 00087: ReduceLROnPlateau reducing learning rate to 1e-07.\n",
      "Epoch 88/1000\n",
      "600/600 [==============================] - 612s 1s/step - loss: 0.0506 - val_loss: 0.0588\n",
      "\n",
      "Epoch 00088: val_loss did not improve from 0.05875\n",
      "Epoch 89/1000\n",
      "600/600 [==============================] - 612s 1s/step - loss: 0.0506 - val_loss: 0.0588\n",
      "\n",
      "Epoch 00089: val_loss did not improve from 0.05875\n",
      "Epoch 90/1000\n",
      "600/600 [==============================] - 612s 1s/step - loss: 0.0507 - val_loss: 0.0588\n",
      "\n",
      "Epoch 00090: val_loss did not improve from 0.05875\n",
      "Epoch 91/1000\n",
      "600/600 [==============================] - 612s 1s/step - loss: 0.0506 - val_loss: 0.0588\n",
      "\n",
      "Epoch 00091: val_loss did not improve from 0.05875\n",
      "Epoch 92/1000\n",
      "600/600 [==============================] - 612s 1s/step - loss: 0.0506 - val_loss: 0.0588\n",
      "\n",
      "Epoch 00092: val_loss did not improve from 0.05875\n",
      "Epoch 93/1000\n",
      "600/600 [==============================] - 612s 1s/step - loss: 0.0507 - val_loss: 0.0588\n",
      "\n",
      "Epoch 00093: val_loss did not improve from 0.05875\n",
      "Epoch 94/1000\n",
      "600/600 [==============================] - 611s 1s/step - loss: 0.0506 - val_loss: 0.0588\n",
      "\n",
      "Epoch 00094: val_loss did not improve from 0.05875\n",
      "Epoch 95/1000\n",
      "600/600 [==============================] - 612s 1s/step - loss: 0.0507 - val_loss: 0.0588\n",
      "\n",
      "Epoch 00095: val_loss did not improve from 0.05875\n",
      "Epoch 96/1000\n",
      "600/600 [==============================] - 611s 1s/step - loss: 0.0506 - val_loss: 0.0588\n",
      "\n",
      "Epoch 00096: val_loss did not improve from 0.05875\n",
      "Epoch 97/1000\n",
      "600/600 [==============================] - 613s 1s/step - loss: 0.0507 - val_loss: 0.0588\n",
      "\n",
      "Epoch 00097: val_loss did not improve from 0.05875\n",
      "Epoch 98/1000\n",
      "600/600 [==============================] - 612s 1s/step - loss: 0.0506 - val_loss: 0.0587\n",
      "\n",
      "Epoch 00098: val_loss did not improve from 0.05875\n",
      "Epoch 99/1000\n",
      "600/600 [==============================] - 612s 1s/step - loss: 0.0506 - val_loss: 0.0588\n",
      "\n",
      "Epoch 00099: val_loss did not improve from 0.05875\n",
      "Epoch 100/1000\n",
      "600/600 [==============================] - 612s 1s/step - loss: 0.0506 - val_loss: 0.0588\n",
      "\n",
      "Epoch 00100: val_loss did not improve from 0.05875\n",
      "Epoch 101/1000\n",
      "600/600 [==============================] - 612s 1s/step - loss: 0.0506 - val_loss: 0.0588\n",
      "\n",
      "Epoch 00101: val_loss did not improve from 0.05875\n",
      "Epoch 102/1000\n",
      "600/600 [==============================] - 612s 1s/step - loss: 0.0507 - val_loss: 0.0588\n",
      "\n",
      "Epoch 00102: val_loss did not improve from 0.05875\n",
      "Epoch 103/1000\n",
      "600/600 [==============================] - 612s 1s/step - loss: 0.0506 - val_loss: 0.0588\n",
      "\n",
      "Epoch 00103: val_loss did not improve from 0.05875\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.layers import Input, Conv1D, BatchNormalization, Activation, MaxPooling1D, Flatten, TimeDistributed\n",
    "from tensorflow.keras.layers import Dropout, Bidirectional, LSTM, Attention, Multiply, Reshape, Permute, Dense\n",
    "from tensorflow.keras import regularizers\n",
    "from tensorflow.keras.backend import clear_session\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau\n",
    "import pickle\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "\n",
    "for f_num in range(0,1):\n",
    "    T = 200\n",
    "    kernel_size= 8\n",
    "    model_path = 'Models/' + 'trajectories_model_alpha_(12/05) (T = {}, ks = {}, 3_cnn, causal).hdf5'.format(\n",
    "        T, kernel_size)\n",
    "\n",
    "    cb_checkpoint = ModelCheckpoint(filepath=model_path, monitor='val_loss', mode = 'min',\\\n",
    "                                    verbose=1, save_best_only=True)\n",
    "    cb_early_stopping = EarlyStopping(monitor='val_loss', mode = 'min', patience= 20)\n",
    "\n",
    "    # Compile model\n",
    "\n",
    "    rlr = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience= 2, min_lr=1e-7, \\\n",
    "                            verbose=1, min_delta=1e-5)\n",
    "\n",
    "\n",
    "    l1 = 1e-5\n",
    "    l2 = 1e-4\n",
    "    lr = 1e-3\n",
    "    \n",
    "    strides=1\n",
    "    activation='relu'\n",
    "\n",
    "    inputs = Input(shape=(T,2), name='input1')\n",
    "    \n",
    "\n",
    "    \n",
    "  \n",
    "    \n",
    "    x = Conv1D(512, kernel_size=kernel_size, padding = 'causal')(inputs)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Conv1D(512, kernel_size=kernel_size, padding = 'causal')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Conv1D(512, kernel_size=kernel_size, padding = 'causal')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "\n",
    "\n",
    "    x = Bidirectional(LSTM(1024, activation='tanh', return_sequences=True))(x)\n",
    "    x = BatchNormalization()(x)\n",
    "\n",
    "    x = Bidirectional(LSTM(512, activation='tanh', return_sequences=True))(x)\n",
    "    x = BatchNormalization()(x)\n",
    "\n",
    "    x = Bidirectional(LSTM(256, activation='tanh', return_sequences=False))(x)\n",
    "\n",
    "    \n",
    "    x = BatchNormalization()(x)\n",
    "    x = Dropout(0.4)(x)\n",
    "    \n",
    "    outputs = Dense(\n",
    "            200,\n",
    "            activation=activation,\n",
    "            kernel_regularizer=regularizers.l1_l2(l1=l1, l2=l2)\n",
    "        )(x)\n",
    "\n",
    "    model = Model(inputs=inputs, outputs=outputs)\n",
    "    model.summary()\n",
    "\n",
    "\n",
    "    model.compile(optimizer= Adam(lr),\n",
    "                      loss='mae')\n",
    "\n",
    " \n",
    "\n",
    "    val_loss_history = ValLossHistory()\n",
    "    \n",
    "    history = model.fit(train_input, train_output, epochs = 1000, batch_size= 500, verbose=1, \\\n",
    "                               validation_data=(val_input, val_output),callbacks=[cb_checkpoint, cb_early_stopping, rlr, val_loss_history])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "aabf461b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-12T04:39:55.386357Z",
     "start_time": "2024-12-12T04:39:55.260674Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7f4b4c587d00>]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(val_loss_history.val_loss_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f0196cf3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-12T04:41:03.575948Z",
     "start_time": "2024-12-12T04:41:03.570690Z"
    }
   },
   "outputs": [],
   "source": [
    "with open('learning curve(val_loss)/trajectories_model_alpha_(12.05) (T = {}, ks = {}, 3_cnn, causal)'.format(T, tau), 'wb') as f:\n",
    "    pickle.dump(val_loss_history.val_loss_list, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9ed60a48",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-03T06:59:50.226596Z",
     "start_time": "2025-03-03T06:59:50.221902Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "T = 200\n",
    "kernel_size = 8\n",
    "with open('learning curve(val_loss)/trajectories_model_alpha_(12.05) (T = {}, ks = {}, 1_cnn, causal)'.format(T, kernel_size), mode='rb') as f:\n",
    "    val_loss1 = pickle.load(f)\n",
    "    \n",
    "with open('learning curve(val_loss)/trajectories_model_alpha_(12.05) (T = {}, ks = {}, 2_cnn, causal)'.format(T, kernel_size), mode='rb') as f:\n",
    "    val_loss2 = pickle.load(f)\n",
    "    \n",
    "with open('learning curve(val_loss)/trajectories_model_alpha_(12.05) (T = {}, ks = {}, 3_cnn, causal)'.format(T, kernel_size), mode='rb') as f:\n",
    "    val_loss3 = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2a73a130",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-03T06:59:52.335123Z",
     "start_time": "2025-03-03T06:59:52.139754Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7ffb58bcab50>]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(val_loss1)\n",
    "plt.plot(val_loss2)\n",
    "plt.plot(val_loss3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "75259a66",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-03T07:00:01.028384Z",
     "start_time": "2025-03-03T07:00:01.024047Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.07281647622585297\n",
      "0.06762491911649704\n",
      "0.05874839425086975\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "print(np.min(val_loss1))\n",
    "print(np.min(val_loss2))\n",
    "print(np.min(val_loss3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e4c7ccf1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-03T07:00:30.666054Z",
     "start_time": "2025-03-03T07:00:30.661798Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "with open('learning curve(val_loss)/trajectories_model_alpha_(12.05) (T = {}, ks = {}, 1_cnn)'.format(T, kernel_size), mode='rb') as f:\n",
    "    val_loss1 = pickle.load(f)\n",
    "    \n",
    "with open('learning curve(val_loss)/trajectories_model_alpha_(12.05) (T = {}, ks = {}, 2_cnn)'.format(T, kernel_size), mode='rb') as f:\n",
    "    val_loss2 = pickle.load(f)\n",
    "    \n",
    "with open('learning curve(val_loss)/trajectories_model_alpha_(12.05) (T = {}, ks = {}, 3_cnn)'.format(T, kernel_size), mode='rb') as f:\n",
    "    val_loss3 = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b2f62bd6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-03T07:00:32.329171Z",
     "start_time": "2025-03-03T07:00:32.153086Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7ffb58a422e0>]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(val_loss1)\n",
    "plt.plot(val_loss2)\n",
    "plt.plot(val_loss3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0b6dd341",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-03T07:00:33.240934Z",
     "start_time": "2025-03-03T07:00:33.235893Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.06848110258579254\n",
      "0.05975636467337608\n",
      "0.0694260448217392\n"
     ]
    }
   ],
   "source": [
    "print(np.min(val_loss1))\n",
    "print(np.min(val_loss2))\n",
    "print(np.min(val_loss3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d1d300c",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2025-03-03T06:59:14.131Z"
    }
   },
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.layers import Input, Conv1D, BatchNormalization, Activation, MaxPooling1D, Flatten, TimeDistributed\n",
    "from tensorflow.keras.layers import Dropout, Bidirectional, LSTM, Attention, Multiply, Reshape, Permute, Dense\n",
    "from tensorflow.keras import regularizers\n",
    "from tensorflow.keras.backend import clear_session\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau\n",
    "import pickle\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "\n",
    "T = 200\n",
    "tau = 3\n",
    "kernel_size = 8\n",
    "model_path = 'Models/' + 'trajectories_model_alpha_(12/05) (T = {}, ks = {}, 3_cnn).hdf5'.format(T, kernel_size, kernel_size)\n",
    "model = keras.models.load_model(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "842e1bd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c5bfe5fd",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-30T07:05:32.722818Z",
     "start_time": "2024-11-30T07:05:28.167365Z"
    }
   },
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.layers import Input, Conv1D, BatchNormalization, Activation, MaxPooling1D, Flatten, TimeDistributed\n",
    "from tensorflow.keras.layers import Dropout, Bidirectional, LSTM, Attention, Multiply, Reshape, Permute, Dense\n",
    "from tensorflow.keras import regularizers\n",
    "from tensorflow.keras.backend import clear_session\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau\n",
    "import pickle\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "\n",
    "T = 200\n",
    "tau = 3\n",
    "kernel_size = 8\n",
    "model_path = 'Models/' + 'model_alpha_(11/26) (T = {}, tau = {}, ks = {}).hdf5'.format(T, tau, kernel_size)\n",
    "model = keras.models.load_model(model_path)\n",
    "\n",
    "\n",
    "cb_checkpoint = ModelCheckpoint(filepath=model_path, monitor='val_loss', mode = 'min',\\\n",
    "                                    verbose=1, save_best_only=True)\n",
    "cb_early_stopping = EarlyStopping(monitor='val_loss', mode = 'min', patience= 20)\n",
    "\n",
    "    # Compile model\n",
    "\n",
    "rlr = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience= 2, min_lr=1e-7, \\\n",
    "                            verbose=1, min_delta=1e-5)\n",
    "\n",
    "\n",
    "lr = 7.812500371073838e-06\n",
    "model.compile(optimizer= Adam(lr),\n",
    "                      loss='mae')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2b6348bf",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-30T07:13:28.806042Z",
     "start_time": "2024-11-30T07:05:41.825116Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1000\n",
      "474/600 [======================>.......] - ETA: 2:01 - loss: 0.0551"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-9cb027f8f7e7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m history = model.fit(train_input, train_output, epochs = 1000, batch_size= 500, verbose=1,\\\n\u001b[0m\u001b[1;32m      2\u001b[0m                     validation_data=(val_input, val_output),callbacks=[cb_checkpoint, cb_early_stopping, rlr])\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1186\u001b[0m               \u001b[0mlogs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtmp_logs\u001b[0m  \u001b[0;31m# No error, now safe to assign to logs.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1187\u001b[0m               \u001b[0mend_step\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstep\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep_increment\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1188\u001b[0;31m               \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mend_step\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1189\u001b[0m               \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop_training\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1190\u001b[0m                 \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/tensorflow/python/keras/callbacks.py\u001b[0m in \u001b[0;36mon_train_batch_end\u001b[0;34m(self, batch, logs)\u001b[0m\n\u001b[1;32m    455\u001b[0m     \"\"\"\n\u001b[1;32m    456\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_should_call_train_batch_hooks\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 457\u001b[0;31m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_batch_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mModeKeys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTRAIN\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'end'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    458\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    459\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mon_test_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/tensorflow/python/keras/callbacks.py\u001b[0m in \u001b[0;36m_call_batch_hook\u001b[0;34m(self, mode, hook, batch, logs)\u001b[0m\n\u001b[1;32m    315\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_batch_begin_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    316\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'end'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 317\u001b[0;31m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_batch_end_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    318\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    319\u001b[0m       \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Unrecognized hook: {}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/tensorflow/python/keras/callbacks.py\u001b[0m in \u001b[0;36m_call_batch_end_hook\u001b[0;34m(self, mode, batch, logs)\u001b[0m\n\u001b[1;32m    335\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_batch_times\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_time\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    336\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 337\u001b[0;31m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_batch_hook_helper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhook_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    338\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    339\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_batch_times\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_batches_for_timing_check\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/tensorflow/python/keras/callbacks.py\u001b[0m in \u001b[0;36m_call_batch_hook_helper\u001b[0;34m(self, hook_name, batch, logs)\u001b[0m\n\u001b[1;32m    373\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mcallback\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    374\u001b[0m       \u001b[0mhook\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcallback\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 375\u001b[0;31m       \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    376\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    377\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_timing\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/tensorflow/python/keras/callbacks.py\u001b[0m in \u001b[0;36mon_train_batch_end\u001b[0;34m(self, batch, logs)\u001b[0m\n\u001b[1;32m   1027\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1028\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mon_train_batch_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1029\u001b[0;31m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_batch_update_progbar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1030\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1031\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mon_test_batch_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/tensorflow/python/keras/callbacks.py\u001b[0m in \u001b[0;36m_batch_update_progbar\u001b[0;34m(self, batch, logs)\u001b[0m\n\u001b[1;32m   1099\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mverbose\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1100\u001b[0m       \u001b[0;31m# Only block async when verbose = 1.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1101\u001b[0;31m       \u001b[0mlogs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msync_to_numpy_or_python_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1102\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprogbar\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mseen\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfinalize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1103\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/tensorflow/python/keras/utils/tf_utils.py\u001b[0m in \u001b[0;36msync_to_numpy_or_python_type\u001b[0;34m(tensors)\u001b[0m\n\u001b[1;32m    517\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mt\u001b[0m  \u001b[0;31m# Don't turn ragged or sparse tensors to NumPy.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    518\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 519\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mnest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap_structure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_to_single_numpy_or_python_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    520\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    521\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/tensorflow/python/util/nest.py\u001b[0m in \u001b[0;36mmap_structure\u001b[0;34m(func, *structure, **kwargs)\u001b[0m\n\u001b[1;32m    865\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    866\u001b[0m   return pack_sequence_as(\n\u001b[0;32m--> 867\u001b[0;31m       \u001b[0mstructure\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mentries\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    868\u001b[0m       expand_composites=expand_composites)\n\u001b[1;32m    869\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/tensorflow/python/util/nest.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    865\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    866\u001b[0m   return pack_sequence_as(\n\u001b[0;32m--> 867\u001b[0;31m       \u001b[0mstructure\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mentries\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    868\u001b[0m       expand_composites=expand_composites)\n\u001b[1;32m    869\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/tensorflow/python/keras/utils/tf_utils.py\u001b[0m in \u001b[0;36m_to_single_numpy_or_python_type\u001b[0;34m(t)\u001b[0m\n\u001b[1;32m    513\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_to_single_numpy_or_python_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    514\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 515\u001b[0;31m       \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    516\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    517\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mt\u001b[0m  \u001b[0;31m# Don't turn ragged or sparse tensors to NumPy.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36mnumpy\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1092\u001b[0m     \"\"\"\n\u001b[1;32m   1093\u001b[0m     \u001b[0;31m# TODO(slebedev): Consider avoiding a copy for non-CPU or remote tensors.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1094\u001b[0;31m     \u001b[0mmaybe_arr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1095\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mmaybe_arr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmaybe_arr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mmaybe_arr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1096\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36m_numpy\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1058\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1059\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1060\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_numpy_internal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1061\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1062\u001b[0m       \u001b[0msix\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_from\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_status_to_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "history = model.fit(train_input, train_output, epochs = 1000, batch_size= 500, verbose=1,\\\n",
    "                    validation_data=(val_input, val_output),callbacks=[cb_checkpoint, cb_early_stopping, rlr])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c087edd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from tensorflow.keras.models import Model\n",
    "# from tensorflow.keras.optimizers import Adam\n",
    "# from tensorflow.keras.layers import Input, Conv1D, BatchNormalization, Activation, MaxPooling1D, Flatten, TimeDistributed\n",
    "# from tensorflow.keras.layers import Dropout, Bidirectional, LSTM, Attention, Multiply, Reshape, Permute, Dense\n",
    "# from tensorflow.keras import regularizers\n",
    "# from tensorflow.keras.backend import clear_session\n",
    "# from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau\n",
    "# import pickle\n",
    "# import numpy as np\n",
    "# import matplotlib.pyplot as plt\n",
    "# import tensorflow as tf\n",
    "\n",
    "for f_num in range(0,1):\n",
    "\n",
    "\n",
    "    model_path = 'Models/' + 'model_alpha_(11/26) (T = {}, tau = {}, ks = 3).hdf5'.format(T, tau)\n",
    "\n",
    "    cb_checkpoint = ModelCheckpoint(filepath=model_path, monitor='val_loss', mode = 'min',\\\n",
    "                                    verbose=1, save_best_only=True)\n",
    "    cb_early_stopping = EarlyStopping(monitor='val_loss', mode = 'min', patience= 20)\n",
    "\n",
    "    # Compile model\n",
    "\n",
    "    rlr = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience= 2, min_lr=1e-7, \\\n",
    "                            verbose=1, min_delta=1e-5)\n",
    "\n",
    "\n",
    "    l1 = 1e-5\n",
    "    l2 = 1e-4\n",
    "    lr = 1e-3\n",
    "    kernel_size= 3\n",
    "    strides=1\n",
    "    activation='relu'\n",
    "\n",
    "    inputs = Input(shape=(T,2), name='input1')\n",
    "    \n",
    "\n",
    "    \n",
    "  \n",
    "    \n",
    "    x = Conv1D(512, kernel_size=kernel_size, padding = 'same')(inputs)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Conv1D(512, kernel_size=kernel_size, padding = 'same')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Conv1D(512, kernel_size=kernel_size, padding = 'same')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "\n",
    "\n",
    "    x = Bidirectional(LSTM(1024, activation='tanh', return_sequences=True))(x)\n",
    "    x = BatchNormalization()(x)\n",
    "\n",
    "    x = Bidirectional(LSTM(512, activation='tanh', return_sequences=True))(x)\n",
    "    x = BatchNormalization()(x)\n",
    "\n",
    "    x = Bidirectional(LSTM(256, activation='tanh', return_sequences=False))(x)\n",
    "\n",
    "    \n",
    "    x = BatchNormalization()(x)\n",
    "    x = Dropout(0.4)(x)\n",
    "    \n",
    "    outputs = Dense(\n",
    "            200,\n",
    "            activation=activation,\n",
    "            kernel_regularizer=regularizers.l1_l2(l1=l1, l2=l2)\n",
    "        )(x)\n",
    "\n",
    "    model = Model(inputs=inputs, outputs=outputs)\n",
    "    model.summary()\n",
    "\n",
    "\n",
    "    model.compile(optimizer= Adam(lr),\n",
    "                      loss='mae')\n",
    "\n",
    " \n",
    "\n",
    "\n",
    "    \n",
    "    history = model.fit(train_input, train_output, epochs = 1000, batch_size= 500, verbose=1, \\\n",
    "                               validation_data=(val_input, val_output),callbacks=[cb_checkpoint, cb_early_stopping, rlr])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "624ed7d2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-24T15:47:12.279719Z",
     "start_time": "2024-11-24T15:47:12.272071Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2.90864444, 2.11567332, 3.09260343, 1.87777663, 3.27256026,\n",
       "       4.97038006, 4.97038006, 0.44273808])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_data['single'][0][0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5ebc067a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-24T15:47:13.890133Z",
     "start_time": "2024-11-24T15:47:13.884428Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.5.0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "be60ef0f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-24T15:47:14.199421Z",
     "start_time": "2024-11-24T15:47:14.193655Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.19.5\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "print(np.__version__)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
